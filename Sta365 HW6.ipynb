{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd571444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Laijun Xu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb53f101",
   "metadata": {},
   "source": [
    "**Part I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129a060f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [betas, intercept]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='2800' class='' max='2800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [2800/2800 00:13&lt;00:00 Sampling 4 chains, 1,000 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 200 tune and 500 draw iterations (800 + 2_000 draws total) took 14 seconds.\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "There were 1000 divergences after tuning. Increase `target_accept` or reparameterize.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
      "betas[0]   0.369  0.403  -0.035    0.816      0.200    0.153       5.0   \n",
      "intercept -0.514  0.307  -0.975   -0.136      0.143    0.109       5.0   \n",
      "p[0]       0.552  0.448   0.098    1.000      0.222    0.170       5.0   \n",
      "p[1]       0.558  0.442   0.109    1.000      0.219    0.168       5.0   \n",
      "p[2]       0.556  0.444   0.105    1.000      0.220    0.169       5.0   \n",
      "...          ...    ...     ...      ...        ...      ...       ...   \n",
      "p[6430]    0.527  0.473   0.050    1.000      0.235    0.180       5.0   \n",
      "p[6431]    0.527  0.473   0.050    1.000      0.235    0.180       5.0   \n",
      "p[6432]    0.536  0.464   0.069    1.000      0.230    0.176       5.0   \n",
      "p[6433]    0.534  0.466   0.065    1.000      0.231    0.177       5.0   \n",
      "p[6434]    0.532  0.468   0.060    1.000      0.232    0.178       5.0   \n",
      "\n",
      "           ess_tail  r_hat  \n",
      "betas[0]      303.0   3.39  \n",
      "intercept     328.0   2.29  \n",
      "p[0]          178.0   2.30  \n",
      "p[1]          172.0   2.30  \n",
      "p[2]          146.0   2.30  \n",
      "...             ...    ...  \n",
      "p[6430]       385.0   2.10  \n",
      "p[6431]       385.0   2.10  \n",
      "p[6432]       278.0   2.10  \n",
      "p[6433]       258.0   2.10  \n",
      "p[6434]       551.0   2.10  \n",
      "\n",
      "[6437 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "#1&2.\n",
    "\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import numpy as np\n",
    "\n",
    "file_path = 'Walmart_sales.csv' \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "X = data['Temperature'].values.reshape(-1, 1) \n",
    "y = data['Holiday_Flag'].values  \n",
    "\n",
    "with pm.Model() as logistic_model:\n",
    "\n",
    "    betas = pm.Normal('betas', mu=0, sigma=10, shape=X.shape[1])  \n",
    "    intercept = pm.Normal('intercept', mu=0, sigma=10)  \n",
    "\n",
    "    logits = intercept + pm.math.dot(X, betas)\n",
    "    \n",
    "    p = pm.Deterministic('p', pm.math.sigmoid(logits))\n",
    "    \n",
    "    y_obs = pm.Bernoulli('y_obs', p=p, observed=y)\n",
    "    \n",
    "    trace = pm.sample(500, tune=200, target_accept=0.95)\n",
    "\n",
    "summary = pm.summary(trace)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec8ab2a",
   "metadata": {},
   "source": [
    "3. \n",
    "When choosing priors for a Bayesian model, it is essential to reflect on the level of information available about the parameters in question. Without strong prior information, it is common to opt for weakly informative or non-informative priors, which aim to impose minimal influence on the posterior estimates, allowing the data to speak for themselves.\n",
    "\n",
    "For the 'betas' parameter, which represents the effect of the temperature on the log odds of the outcome (holiday shopping behavior in this case), a normal distribution with a large variance can be considered a weakly informative prior. For example, a 'Normal(0, 10^2)' distribution for 'betas' suggests we believe a priori that most of the mass of the distribution of this effect size lies within about two standard deviations of zero but does not rule out larger values.\n",
    "\n",
    "For the 'intercept' parameter, which represents the baseline log odds of the outcome when the temperature is at zero, a similar approach can be taken. A 'Normal(0, 10^2)' prior for the 'intercept' would reflect a belief that the baseline probability is as likely to be high as it is to be low, without strong evidence in either direction.\n",
    "\n",
    "In summary, the choice of 'Normal(0, 10^2)' for both 'betas' and 'intercept' represents a weakly informative prior that allows the data to largely determine the posterior while still regularizing the estimates to avoid extreme values that are not supported by the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f8b27",
   "metadata": {},
   "source": [
    "**Part II**\n",
    "\n",
    "For ridge regression, the penalized loss function can be written as:\n",
    "$\\text{Ridge Loss} = \\frac{1}{2} \\sum_{i=1}^n (y_i - x_i^T \\beta)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2$\n",
    "\n",
    "Under the Bayesian framework with normal priors on $\\beta$ coefficients, we have:\n",
    "$p(\\beta | y, X) \\propto \\exp \\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_i^T \\beta)^2 \\right) \\exp \\left( -\\frac{1}{2\\tau^2} \\sum_{j=1}^p \\beta_j^2 \\right)$\n",
    "\n",
    "Setting $\\sigma = 1$ and $\\lambda = \\frac{1}{2\\tau^2}$, we have:\n",
    "$p(\\beta | y, X) \\propto \\exp \\left( -\\frac{1}{2} \\sum_{i=1}^n (y_i - x_i^T \\beta)^2 - \\lambda \\sum_{j=1}^p \\beta_j^2 \\right)$\n",
    "\n",
    "For lasso regression, the penalized loss function is given by:\n",
    "$\\text{Lasso Loss} = \\frac{1}{2} \\sum_{i=1}^n (y_i - x_i^T \\beta)^2 + \\lambda \\sum_{j=1}^p |\\beta_j|$\n",
    "\n",
    "In Bayesian terms with Laplace priors on $\\beta$ coefficients:\n",
    "$p(\\beta | y, X) \\propto \\exp \\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_i^T \\beta)^2 \\right) \\exp \\left( -\\frac{\\lambda}{\\sigma} \\sum_{j=1}^p |\\beta_j| \\right)$\n",
    "\n",
    "Setting $\\sigma = 1$ and using the same $\\lambda$ as in lasso, we have:\n",
    "$p(\\beta | y, X) \\propto \\exp \\left( -\\frac{1}{2} \\sum_{i=1}^n (y_i - x_i^T \\beta)^2 - \\lambda \\sum_{j=1}^p |\\beta_j| \\right)$\n",
    "\n",
    "The statement to be written and understood is: \"Bayesians do not optimize posterior distributions, they sample from them; but, the posterior distributions are nonetheless 'regularizations' of the likelihood through the prior.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc005bbb",
   "metadata": {},
   "source": [
    "**Part III**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c114d085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [alpha, beta, intercept, lambda_]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='8000' class='' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [8000/8000 00:29&lt;00:00 Sampling 4 chains, 2,261 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 29 seconds.\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "There were 2261 divergences after tuning. Increase `target_accept` or reparameterize.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier indices: [2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2117]\n"
     ]
    }
   ],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "file_path = 'Walmart_sales.csv' \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "X = data['Temperature'].values[:, None]  # Predictor\n",
    "y = data['Holiday_Flag'].values  # Response\n",
    "\n",
    "# Robust regression model\n",
    "with pm.Model() as robust_model:\n",
    "\n",
    "    alpha = pm.Gamma('alpha', alpha=0.5, beta=0.5)\n",
    "    beta = pm.Normal('beta', mu=0, sigma=10)\n",
    "    intercept = pm.Normal('intercept', mu=0, sigma=10)\n",
    "    \n",
    "    lambda_ = pm.InverseGamma('lambda_', alpha=alpha, beta=alpha)\n",
    "\n",
    "    y_pred = intercept + beta * X.flatten()\n",
    "    likelihood = pm.StudentT('y', nu=1, mu=y_pred, lam=lambda_, observed=y)\n",
    "\n",
    "    trace = pm.sample(1000, return_inferencedata=True)\n",
    "\n",
    "# Identify outliers based on the posterior of lambda_i\n",
    "lambda_posterior = trace.posterior['lambda_'].data.flatten()\n",
    "outlier = np.where(lambda_posterior < np.percentile(lambda_posterior, 1))[0]\n",
    "\n",
    "print('Outlier indices:', outlier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f508266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
